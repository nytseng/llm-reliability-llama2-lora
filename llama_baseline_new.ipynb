{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import gradio as gr\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "assert (\n",
    "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
    "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-2-7b-hf\"\n",
    "LORA_WEIGHTS = \"models/l2model8\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        load_in_8bit=False,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    # model = PeftModel.from_pretrained(\n",
    "    #     model, LORA_WEIGHTS, torch_dtype=torch.float16, force_download=True, \n",
    "    # )\n",
    "elif device == \"mps\":\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        LORA_WEIGHTS,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "else:\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        LORA_WEIGHTS,\n",
    "        device_map={\"\": device},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method generates a prompt for the given instruction and input\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input: \n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "# Instruction:\n",
    "{instruction}\n",
    "# Input:\n",
    "{input}\n",
    "# Response:\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device != \"cpu\":\n",
    "    model.half()\n",
    "model.eval()\n",
    "if torch.__version__ >= \"2\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    instruction,\n",
    "    input=None,\n",
    "    temperature=1.0,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=15,\n",
    "    **kwargs,\n",
    "):\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        prompt = generate_prompt(instruction, input)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\") \n",
    "        # inputs = tokenizer(prompt, max_length=2048 , return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        return output.split(\"### Response:\")[1].strip()\n",
    "\n",
    "\n",
    "def evaluate_prompt_and_instruction(instruction, input):\n",
    "    return evaluate(instruction, input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_df = pd.read_excel(\"Student Essays Final Annotations.xlsx\")\n",
    "    labels = [\"Potential Energy\", \"Kinetic Energy\", \"Law of Conservation of Energy\"]\n",
    "    input_examples = data_df[\"Essay\"]\n",
    "    with open('predict_output_baseline.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        field = [\"Essay ID\",\"Essay\",\"PE_Predicted\",\"KE_Predicted\",\"LCE_Predicted\", \"PE_Actual\",\"KE_Actual\",\"LCE_Actual\"]\n",
    "        writer.writerow(field)\n",
    "\n",
    "        for row in range(len(input_examples)):\n",
    "            curr_essay = input_examples[row] \n",
    "            predicted = [\"unacceptable\",\"unacceptable\",\"unacceptable\"]\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                instruct = \"You are a junior high school teacher grading the answers for a Physics \" + \\\n",
    "                     \"test. You are given a paragraph written by a student to describe a physics concept. \" + \\\n",
    "                     \"Please say Yes if it defines the concept at the input correctly, otherwise say No. \" + \\\n",
    "                     \"Here is the student paragraph: \\'\" + curr_essay + \"\\'\" \n",
    "                s = evaluate(instruction = instruct, input = label)\n",
    "                \n",
    "                print(\"Expected:\" + label + \",  Response:\", s)\n",
    "                \n",
    "                if \"yes\" in s.lower():\n",
    "                    predicted[i] = \"acceptable\"\n",
    "\n",
    "            writer.writerow([data_df[\"Essay ID\"][row], input, predicted[0], predicted[1], predicted[2], data_df[\"PE\"][row],data_df[\"KE\"][row],data_df[\"LCE\"][row]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
